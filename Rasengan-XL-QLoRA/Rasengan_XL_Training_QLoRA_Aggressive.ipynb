{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Let it begin boys\")\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q diffusers transformers accelerate\n",
        "!pip install -q peft bitsandbytes datasets safetensors\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Verification\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import torch\n",
        "import diffusers\n",
        "import transformers\n",
        "import bitsandbytes\n",
        "\n",
        "print(f\"✓ PyTorch: {torch.__version__}\")\n",
        "print(f\"✓ Diffusers: {diffusers.__version__}\")\n",
        "print(f\"✓ Transformers: {transformers.__version__}\")\n",
        "print(f\"✓ CUDA: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "print(\"good to go!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:18:20.481772Z",
          "iopub.execute_input": "2025-11-16T10:18:20.482033Z",
          "iopub.status.idle": "2025-11-16T10:19:49.515789Z",
          "shell.execute_reply.started": "2025-11-16T10:18:20.482008Z",
          "shell.execute_reply": "2025-11-16T10:19:49.514937Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYNAH2OMWY2N",
        "outputId": "a0efb0aa-21b8-4371-da4e-97fed5e269af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let it begin boys\n",
            "\n",
            "================================================================================\n",
            "Verification\n",
            "================================================================================\n",
            "✓ PyTorch: 2.8.0+cu126\n",
            "✓ Diffusers: 0.35.2\n",
            "✓ Transformers: 4.57.1\n",
            "✓ CUDA: True\n",
            "✓ GPU: Tesla T4\n",
            "good to go!\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# google colab use\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "c0KXsVt0ndTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a66e5b6-69d1-489b-b430-3238fb4c902e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install peft==0.5.0\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:20:10.253620Z",
          "iopub.execute_input": "2025-11-16T10:20:10.254199Z",
          "iopub.status.idle": "2025-11-16T10:20:12.729223Z",
          "shell.execute_reply.started": "2025-11-16T10:20:10.254164Z",
          "shell.execute_reply": "2025-11-16T10:20:12.728413Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLmGe5FtWY2R",
        "outputId": "6089756a-acfc-4029-e81b-b31069b26c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft==0.5.0 in /usr/local/lib/python3.12/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft==0.5.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.5.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.5.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft==0.5.0) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.5.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft==0.5.0) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft==0.5.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from peft==0.5.0) (1.11.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft==0.5.0) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.5.0) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->peft==0.5.0) (0.36.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate->peft==0.5.0) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate->peft==0.5.0) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.5.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate->peft==0.5.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate->peft==0.5.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate->peft==0.5.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate->peft==0.5.0) (2025.10.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.5.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.5.0) (0.22.1)\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"importing stuff\")\n",
        "import os, gc, json, time, math\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, asdict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from accelerate import Accelerator\n",
        "from accelerate.utils import set_seed\n",
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:20:19.413808Z",
          "iopub.execute_input": "2025-11-16T10:20:19.414620Z",
          "iopub.status.idle": "2025-11-16T10:20:20.673444Z",
          "shell.execute_reply.started": "2025-11-16T10:20:19.414588Z",
          "shell.execute_reply": "2025-11-16T10:20:20.672596Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KB3BWdiWY2S",
        "outputId": "bd39469b-7aa0-4d15-9631-2208ee533303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing stuff\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset tool\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:20:26.133705Z",
          "iopub.execute_input": "2025-11-16T10:20:26.134645Z",
          "iopub.status.idle": "2025-11-16T10:20:27.468858Z",
          "shell.execute_reply.started": "2025-11-16T10:20:26.134616Z",
          "shell.execute_reply": "2025-11-16T10:20:27.468038Z"
        },
        "id": "iPJ1TIwPWY2T"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the version info and all\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Device:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:20:32.976437Z",
          "iopub.execute_input": "2025-11-16T10:20:32.977570Z",
          "iopub.status.idle": "2025-11-16T10:20:32.982039Z",
          "shell.execute_reply.started": "2025-11-16T10:20:32.977538Z",
          "shell.execute_reply": "2025-11-16T10:20:32.981309Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHYGkgawWY2U",
        "outputId": "260fa7a1-84a1-4f19-c5c5-2062fa787275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126 | CUDA available: True\n",
            "Device: Tesla T4\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face & PEFT\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    UNet2DConditionModel,\n",
        "    DDPMScheduler,\n",
        "    StableDiffusionXLPipeline\n",
        ")\n",
        "from transformers import CLIPTokenizer, CLIPTextModel, CLIPTextModelWithProjection"
      ],
      "metadata": {
        "id": "ZMRqNQ8Xnrh0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the path on your Drive\n",
        "drive_model_path = \"/content/drive/MyDrive/sdxl-base-1.0\"\n",
        "os.makedirs(drive_model_path, exist_ok=True)\n",
        "print(f\" Downloading and saving base model to: {drive_model_path}\")\n",
        "#define model id\n",
        "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\""
      ],
      "metadata": {
        "id": "G9Deycyrn2bO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7356ca-ae12-4881-9406-0b11e99e839d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Downloading and saving base model to: /content/drive/MyDrive/sdxl-base-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saving Tokenizers...\")\n",
        "CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\").save_pretrained(f\"{drive_model_path}/tokenizer\")\n",
        "CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer_2\").save_pretrained(f\"{drive_model_path}/tokenizer_2\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Saving Scheduler...\")\n",
        "DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\").save_pretrained(f\"{drive_model_path}/scheduler\")\n"
      ],
      "metadata": {
        "id": "6p-LcmXXn2Yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7f66c4-95c5-4053-b655-3cfe2c7d39a8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Tokenizers...\n",
            "Saving Scheduler...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saving Text Encoders...\")\n",
        "CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=torch.float16).save_pretrained(f\"{drive_model_path}/text_encoder\")\n",
        "CLIPTextModelWithProjection.from_pretrained(model_id, subfolder=\"text_encoder_2\", torch_dtype=torch.float16).save_pretrained(f\"{drive_model_path}/text_encoder_2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKHF6RGcBrWy",
        "outputId": "64a1a862-f805-4d24-9ac5-4b7ff8606e0c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Text Encoders...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saving VAE...\")\n",
        "AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32).save_pretrained(f\"{drive_model_path}/vae\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dakxYBOlBvKi",
        "outputId": "cd68a1d5-a790-4d41-b13b-33b711374637"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving VAE...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saving UNet\")\n",
        "UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\", torch_dtype=torch.float16).save_pretrained(f\"{drive_model_path}/unet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3hUW0vsxSa3",
        "outputId": "5a75ab6d-bcf4-482e-dcbe-c163a91cd448"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving UNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")"
      ],
      "metadata": {
        "id": "ddm3587fvlrl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training configurations\n",
        "@dataclass\n",
        "class TraininigConfiguration:\n",
        "  modelId: str = \"/content/drive/MyDrive/sdxl-base-1.0\"\n",
        "  datasetId: str = \"lambdalabs/naruto-blip-captions\"\n",
        "  #establish the output directories where we will have output, logs and checkpoints\n",
        "  outputDir: str = \"/content/drive/MyDrive/Naruto_FineTune/output\"\n",
        "  logsDir: str = \"/content/drive/MyDrive/Naruto_FineTune/logs\"\n",
        "  checkpointsDir: str = \"/content/drive/MyDrive/Naruto_FineTune/checkpoints\"\n",
        "  #Training Hyperparamters\n",
        "  resolution: int = 512\n",
        "  train_batch_size: int = 1 #in docs\n",
        "  gradient_accumulation_steps: int = 4\n",
        "  num_train_epochs: int = 3\n",
        "  learning_rate: float = 1e-4\n",
        "  learning_rate_warmup_steps: int = 10\n",
        "  learning_rate_scheduler_type: str = \"cosine\"\n",
        "\n",
        "\n",
        "  # Optimizations\n",
        "  mixed_precision: str = \"fp16\"\n",
        "  grad_checkpointing : bool = True\n",
        "  use_8bit_adam: bool = True\n",
        "  max_grad_norm: float = 1.0\n",
        "\n",
        "  #For a quick dry run let us set some dataset limits that will be used later\n",
        "  max_train_samples: int = 150\n",
        "  seed: int = 42\n",
        "\n",
        "  #checkpoints\n",
        "  save_every_step = 250\n",
        "\n",
        "  #peft;\n",
        "  # PEFT (LoRA/DoRA)\n",
        "  lora_rank: int = 32\n",
        "  lora_alpha: int = 32\n",
        "  lora_dropout: float = 0.1\n",
        "  lora_target_modules: tuple = (\"to_q\",\"to_k\",\"to_v\",\"to_out.0\",\"add_k_proj\", \"add_v_proj\",\"ff.net.0.proj\", \"ff.net.2\")\n",
        "\n",
        "  #debug\n",
        "  debug: bool = True\n",
        "  dry_run_steps: int = 15\n",
        "  mini_dataset_mode: bool = False #do set to false when you have to for the entire dataset for the final fine tuning\n",
        "\n",
        "config = TraininigConfiguration()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:21:01.383898Z",
          "iopub.execute_input": "2025-11-16T10:21:01.384440Z",
          "iopub.status.idle": "2025-11-16T10:21:01.391906Z",
          "shell.execute_reply.started": "2025-11-16T10:21:01.384417Z",
          "shell.execute_reply": "2025-11-16T10:21:01.390950Z"
        },
        "id": "50zHd4EhWY2V"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset and dataloader prep\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import InterpolationMode"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:21:12.804015Z",
          "iopub.execute_input": "2025-11-16T10:21:12.804597Z",
          "iopub.status.idle": "2025-11-16T10:21:12.808070Z",
          "shell.execute_reply.started": "2025-11-16T10:21:12.804573Z",
          "shell.execute_reply": "2025-11-16T10:21:12.807350Z"
        },
        "id": "YU85oyWoWY2V"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(config.datasetId, split = \"train\")\n",
        "if config.mini_dataset_mode:\n",
        "    print(f\"Training using the mini dataset as the mini dataset mode is set to true in the config and we weiil use the first {config.max_train_samples} samples out of {len(ds)}\")\n",
        "    ds = ds.select(range(config.max_train_samples))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:21:19.434294Z",
          "iopub.execute_input": "2025-11-16T10:21:19.434978Z",
          "iopub.status.idle": "2025-11-16T10:21:27.769241Z",
          "shell.execute_reply.started": "2025-11-16T10:21:19.434956Z",
          "shell.execute_reply": "2025-11-16T10:21:27.768681Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvtGLI74WY2W",
        "outputId": "13454ceb-7304-4f20-b67a-729513a8a374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "if config.max_train_samples is not None:\n",
        "    raw_ds = ds.select(range(min(config.max_train_samples, len(ds))))\n",
        "print(f\"Dataset size: {len(ds)}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:21:38.004204Z",
          "iopub.execute_input": "2025-11-16T10:21:38.005234Z",
          "iopub.status.idle": "2025-11-16T10:21:38.012149Z",
          "shell.execute_reply.started": "2025-11-16T10:21:38.005199Z",
          "shell.execute_reply": "2025-11-16T10:21:38.011389Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOpOODnPWY2W",
        "outputId": "18016947-66b8-41e8-87f7-71a6f8a0f5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 1221\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "example_of_dataset = ds[0]\n",
        "print('it looks something like this', example_of_dataset)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:21:49.195875Z",
          "iopub.execute_input": "2025-11-16T10:21:49.196126Z",
          "iopub.status.idle": "2025-11-16T10:21:49.258224Z",
          "shell.execute_reply.started": "2025-11-16T10:21:49.196107Z",
          "shell.execute_reply": "2025-11-16T10:21:49.257504Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoD6-eljWY2X",
        "outputId": "715e55a4-593e-47d8-e03e-049eedd5ee8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it looks something like this {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1080x1080 at 0x78D318BA79B0>, 'text': 'a man with dark hair and brown eyes'}\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "print('it looks something like this', example_of_dataset.keys())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:21:56.853699Z",
          "iopub.execute_input": "2025-11-16T10:21:56.854459Z",
          "iopub.status.idle": "2025-11-16T10:21:56.858593Z",
          "shell.execute_reply.started": "2025-11-16T10:21:56.854431Z",
          "shell.execute_reply": "2025-11-16T10:21:56.857799Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvycYvjRWY2X",
        "outputId": "c742511c-1dc0-4f87-c8e9-f4d647ba603f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it looks something like this dict_keys(['image', 'text'])\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "print('it looks something like this', example_of_dataset.get('text'))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:22:03.133773Z",
          "iopub.execute_input": "2025-11-16T10:22:03.134081Z",
          "iopub.status.idle": "2025-11-16T10:22:03.138073Z",
          "shell.execute_reply.started": "2025-11-16T10:22:03.134062Z",
          "shell.execute_reply": "2025-11-16T10:22:03.137367Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1oMvfvTWY2X",
        "outputId": "b7068363-9069-4e0d-c9ee-2ca79b1ea43b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it looks something like this a man with dark hair and brown eyes\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizers\n",
        "tokenizer_one = CLIPTokenizer.from_pretrained(config.modelId, subfolder=\"tokenizer\")\n",
        "tokenizer_two = CLIPTokenizer.from_pretrained(config.modelId, subfolder=\"tokenizer_2\")\n",
        "print(\"Tokenizers loaded.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:25:30.584281Z",
          "iopub.execute_input": "2025-11-16T10:25:30.584963Z",
          "iopub.status.idle": "2025-11-16T10:25:31.556394Z",
          "shell.execute_reply.started": "2025-11-16T10:25:30.584933Z",
          "shell.execute_reply": "2025-11-16T10:25:31.555562Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsWuzKT7WY2Y",
        "outputId": "da0f922c-bb96-4a2f-ee3a-f00766a64377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizers loaded.\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset class\n",
        "class DatasetOfNaruto(Dataset):\n",
        "  def __init__(self,hf_dataset,tokenizer_one,tokenizer_two, resolution=512):\n",
        "    self.ds = hf_dataset\n",
        "    self.tokenizer_1 = tokenizer_one\n",
        "    self.tokenizer_2 = tokenizer_two\n",
        "    self.resolution= resolution\n",
        "    self.transform = T.Compose([\n",
        "      T.Resize((resolution, resolution), interpolation=InterpolationMode.BILINEAR),\n",
        "      T.CenterCrop((resolution, resolution)),\n",
        "      T.RandomHorizontalFlip(p=0.5),\n",
        "      T.ToTensor(),\n",
        "      T.Normalize([0.5]*3, [0.5]*3)\n",
        "    ])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ds)\n",
        "\n",
        "  def tokenizer_captions(self,text):\n",
        "    tok_caption_1 = self.tokenizer_1(text, padding=\"max_length\", truncation=True, max_length=self.tokenizer_1.model_max_length, return_tensors=\"pt\")\n",
        "    tok_caption_2 = self.tokenizer_2(text, padding=\"max_length\", truncation=True, max_length=self.tokenizer_2.model_max_length, return_tensors=\"pt\")\n",
        "    return tok_caption_1.input_ids.squeeze(0), tok_caption_2.input_ids.squeeze(0)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    item = self.ds[idx]\n",
        "    img = item[\"image\"]\n",
        "    pixel_values = self.transform(img)\n",
        "    caption = item.get(\"text\") or item.get(\"caption\") or item.get(\"title\") or \"\"\n",
        "    ids1, ids2 = self.tokenizer_captions(caption)\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids_one\": ids1, \"input_ids_two\": ids2, \"caption\": caption}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:25:33.583863Z",
          "iopub.execute_input": "2025-11-16T10:25:33.584452Z",
          "iopub.status.idle": "2025-11-16T10:25:33.595014Z",
          "shell.execute_reply.started": "2025-11-16T10:25:33.584428Z",
          "shell.execute_reply": "2025-11-16T10:25:33.594012Z"
        },
        "id": "jAu8hvT5WY2Y"
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset and dataloader\n",
        "train_ds = DatasetOfNaruto(ds, tokenizer_one, tokenizer_two, resolution=config.resolution)\n",
        "train_loader = DataLoader(train_ds, batch_size=config.train_batch_size, shuffle = True, num_workers = 2, pin_memory=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:25:33.883973Z",
          "iopub.execute_input": "2025-11-16T10:25:33.884754Z",
          "iopub.status.idle": "2025-11-16T10:25:33.914768Z",
          "shell.execute_reply.started": "2025-11-16T10:25:33.884721Z",
          "shell.execute_reply": "2025-11-16T10:25:33.914006Z"
        },
        "id": "hA9oWQXvWY2Z"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataloader ready. Number of batches per epoch:\", len(train_loader))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:25:35.023889Z",
          "iopub.execute_input": "2025-11-16T10:25:35.024621Z",
          "iopub.status.idle": "2025-11-16T10:25:35.028733Z",
          "shell.execute_reply.started": "2025-11-16T10:25:35.024595Z",
          "shell.execute_reply": "2025-11-16T10:25:35.027870Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG6-KZzqWY2Z",
        "outputId": "e5720c9e-3cab-4377-ea58-b700b2eca02e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloader ready. Number of batches per epoch: 1221\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "print(\"Batch keys:\", batch.keys())\n",
        "print(\"Pixel values shape:\", batch[\"pixel_values\"].shape)\n",
        "print(\"Caption (first):\", batch[\"caption\"][0])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:25:35.303899Z",
          "iopub.execute_input": "2025-11-16T10:25:35.304619Z",
          "iopub.status.idle": "2025-11-16T10:25:35.666291Z",
          "shell.execute_reply.started": "2025-11-16T10:25:35.304588Z",
          "shell.execute_reply": "2025-11-16T10:25:35.665403Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcAeoSa8WY2Z",
        "outputId": "f4b86573-882a-413a-c0f2-8f81d419501f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch keys: dict_keys(['pixel_values', 'input_ids_one', 'input_ids_two', 'caption'])\n",
            "Pixel values shape: torch.Size([1, 3, 512, 512])\n",
            "Caption (first): a woman with long black hair and a pink shirt\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the models\n",
        "from diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler\n",
        "from transformers import CLIPTextModel, CLIPTextModelWithProjection"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:25:36.544458Z",
          "iopub.execute_input": "2025-11-16T10:25:36.545078Z",
          "iopub.status.idle": "2025-11-16T10:25:36.549786Z",
          "shell.execute_reply.started": "2025-11-16T10:25:36.545039Z",
          "shell.execute_reply": "2025-11-16T10:25:36.548982Z"
        },
        "id": "Xs9labPEWY2Z"
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "#load the text encoders\n",
        "text_encoder_1 = CLIPTextModel.from_pretrained(config.modelId, subfolder=\"text_encoder\", torch_dtype = torch.float16, device_map=\"cpu\")\n",
        "text_encoder_2 = CLIPTextModel.from_pretrained(config.modelId, subfolder=\"text_encoder_2\", torch_dtype = torch.float16, device_map=\"cpu\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:25:36.913775Z",
          "iopub.execute_input": "2025-11-16T10:25:36.914607Z",
          "iopub.status.idle": "2025-11-16T10:25:44.552237Z",
          "shell.execute_reply.started": "2025-11-16T10:25:36.914582Z",
          "shell.execute_reply": "2025-11-16T10:25:44.551405Z"
        },
        "id": "-NPwJdzOWY2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926bc318-5b29-48b6-b393-2e4c31a892d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "acc = Accelerator(\n",
        "    gradient_accumulation_steps = config.gradient_accumulation_steps,\n",
        "    mixed_precision=config.mixed_precision,\n",
        ")"
      ],
      "metadata": {
        "id": "ofAfI9jaHQ86"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = AutoencoderKL.from_pretrained(\n",
        "    config.modelId,\n",
        "    subfolder=\"vae\",\n",
        "    torch_dtype=torch.float32,   # MUST be float32\n",
        ")\n",
        "vae.requires_grad_(False)\n",
        "vae.to(acc.device) #to be noted that this VAE is the worst piece of thing ever - needs to be trained on the gpu but make sure that the dtype is float32 and not 16 -> this will cause mismatch error in data types if you do this -> personal notes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:49:42.501069Z",
          "iopub.execute_input": "2025-11-16T10:49:42.501374Z",
          "iopub.status.idle": "2025-11-16T10:49:42.825026Z",
          "shell.execute_reply.started": "2025-11-16T10:49:42.501356Z",
          "shell.execute_reply": "2025-11-16T10:49:42.824398Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3bp3xCsWY2a",
        "outputId": "e27a34b0-e196-43ff-c0c2-c00483b5941e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoencoderKL(\n",
              "  (encoder): Encoder(\n",
              "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (down_blocks): ModuleList(\n",
              "      (0): DownEncoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0-1): 2 x ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "        (downsamplers): ModuleList(\n",
              "          (0): Downsample2D(\n",
              "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): DownEncoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (1): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "        (downsamplers): ModuleList(\n",
              "          (0): Downsample2D(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): DownEncoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (1): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "        (downsamplers): ModuleList(\n",
              "          (0): Downsample2D(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): DownEncoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0-1): 2 x ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (mid_block): UNetMidBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0): Attention(\n",
              "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_out): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "    (conv_act): SiLU()\n",
              "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (up_blocks): ModuleList(\n",
              "      (0-1): 2 x UpDecoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0-2): 3 x ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "        (upsamplers): ModuleList(\n",
              "          (0): Upsample2D(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): UpDecoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (1-2): 2 x ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "        (upsamplers): ModuleList(\n",
              "          (0): Upsample2D(\n",
              "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): UpDecoderBlock2D(\n",
              "        (resnets): ModuleList(\n",
              "          (0): ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (1-2): 2 x ResnetBlock2D(\n",
              "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (nonlinearity): SiLU()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (mid_block): UNetMidBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0): Attention(\n",
              "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (to_out): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "    (conv_act): SiLU()\n",
              "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DConditionModel\n",
        "import torch\n",
        "\n",
        "print(\" Loading UNet in 4-bit QLoRA mode...\")\n",
        "\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    config.modelId,\n",
        "    subfolder=\"unet\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# --- MANUAL PREPARATION FOR k-BIT TRAINING ---\n",
        "# 1. Cast LayerNorms to float32 for stability (Critical for QLoRA)\n",
        "for name, param in unet.named_parameters():\n",
        "    if \"norm\" in name:\n",
        "        param.data = param.data.to(torch.float32)\n",
        "\n",
        "# 2. Enable Gradient Checkpointing (Saves massive VRAM)\n",
        "if config.grad_checkpointing:\n",
        "    unet.enable_gradient_checkpointing()\n",
        "\n",
        "# 3. Ensure we can calculate gradients\n",
        "# (Quantized models freeze weights, so we need to unfreeze something to start)\n",
        "# We don't strictly need 'get_input_embeddings' logic for UNet.\n",
        "# The PEFT library will handle the LoRA injection next.\n",
        "print(\" UNet loaded in 4-bit and manually prepared!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:26:19.553954Z",
          "iopub.execute_input": "2025-11-16T10:26:19.554671Z",
          "iopub.status.idle": "2025-11-16T10:27:12.162014Z",
          "shell.execute_reply.started": "2025-11-16T10:26:19.554645Z",
          "shell.execute_reply": "2025-11-16T10:27:12.161208Z"
        },
        "id": "KZIOe2R8WY2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adbb7205-8a65-46e0-af1c-c4cfa6084284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading UNet in 4-bit QLoRA mode...\n",
            " UNet loaded in 4-bit and manually prepared!\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "#noise scheduler\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(config.modelId, subfolder=\"scheduler\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:27:12.163562Z",
          "iopub.execute_input": "2025-11-16T10:27:12.163805Z",
          "iopub.status.idle": "2025-11-16T10:27:12.346497Z",
          "shell.execute_reply.started": "2025-11-16T10:27:12.163789Z",
          "shell.execute_reply": "2025-11-16T10:27:12.345750Z"
        },
        "id": "NzsAMRLjWY2a"
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": [
        "#now we will freeze te1, te1 and vae\n",
        "for p in text_encoder_1.parameters():\n",
        "  p.requires_grad=False\n",
        "for p in text_encoder_2.parameters():\n",
        "  p.requires_grad=False\n",
        "\n",
        "for p in vae.parameters():\n",
        "  p.requires_grad=False\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:27:23.283877Z",
          "iopub.execute_input": "2025-11-16T10:27:23.284160Z",
          "iopub.status.idle": "2025-11-16T10:27:23.291336Z",
          "shell.execute_reply.started": "2025-11-16T10:27:23.284138Z",
          "shell.execute_reply": "2025-11-16T10:27:23.290530Z"
        },
        "id": "yiWSR1WHWY2a"
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:27:29.744277Z",
          "iopub.execute_input": "2025-11-16T10:27:29.744998Z",
          "iopub.status.idle": "2025-11-16T10:27:29.749347Z",
          "shell.execute_reply.started": "2025-11-16T10:27:29.744965Z",
          "shell.execute_reply": "2025-11-16T10:27:29.748337Z"
        },
        "id": "o66VlFlpWY2a"
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "def cuda_mem_report(prefix=\"\"):\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"{prefix} Allocated: {torch.cuda.memory_allocated() / 1e9:.3f} GB | Reserved: {torch.cuda.memory_reserved() / 1e9:.3f} GB\")\n",
        "    else:\n",
        "        print(f\"{prefix} No CUDA available\")\n",
        "\n",
        "cuda_mem_report(\"After imports:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9-hxjfNHnw0",
        "outputId": "aafe9ee4-ffe5-4653-d21f-c2d0071534df"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After imports: Allocated: 2.315 GB | Reserved: 2.345 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(\"Moving Unet to GPU\")\n",
        "    unet.to(device=device)\n",
        "    cuda_mem_report(\"After moving UNet:\")\n",
        "except RuntimeError as e:\n",
        "    print(\"Well damn, out of memory error!!!!!!!!!!\", e)\n",
        "    unet.to(device=\"cpu\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:27:36.053993Z",
          "iopub.execute_input": "2025-11-16T10:27:36.054337Z",
          "iopub.status.idle": "2025-11-16T10:27:37.940471Z",
          "shell.execute_reply.started": "2025-11-16T10:27:36.054312Z",
          "shell.execute_reply": "2025-11-16T10:27:37.939663Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giUrHsVIWY2b",
        "outputId": "ab5a2daa-da5e-4e35-e30e-82d9b49902da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving Unet to GPU\n",
            "After moving UNet: Allocated: 2.315 GB | Reserved: 2.345 GB\n"
          ]
        }
      ],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "if config.grad_checkpointing:\n",
        "    try:\n",
        "        unet.enable_gradient_checkpointing()\n",
        "        print(\"Gradient Checpointting is enabled on the UNet - Nice!\")\n",
        "    except Exception as e:\n",
        "        print(\"Not enabled!!!!!!:\", e)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:27:43.083585Z",
          "iopub.execute_input": "2025-11-16T10:27:43.084137Z",
          "iopub.status.idle": "2025-11-16T10:27:43.094658Z",
          "shell.execute_reply.started": "2025-11-16T10:27:43.084105Z",
          "shell.execute_reply": "2025-11-16T10:27:43.093906Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8hW7AOBWY2b",
        "outputId": "ab66e329-d827-446d-8c9d-9a1066b65826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Checpointting is enabled on the UNet - Nice!\n"
          ]
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_parameters = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters are :\", trainable_parameters)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:28:06.184264Z",
          "iopub.execute_input": "2025-11-16T10:28:06.184935Z",
          "iopub.status.idle": "2025-11-16T10:28:06.193782Z",
          "shell.execute_reply.started": "2025-11-16T10:28:06.184911Z",
          "shell.execute_reply": "2025-11-16T10:28:06.193010Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCFjJFZ9WY2b",
        "outputId": "8a2859cb-e925-430b-a1b0-3b21f9513753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters are : 333860164\n"
          ]
        }
      ],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:28:20.005649Z",
          "iopub.execute_input": "2025-11-16T10:28:20.006363Z",
          "iopub.status.idle": "2025-11-16T10:28:20.009964Z",
          "shell.execute_reply.started": "2025-11-16T10:28:20.006337Z",
          "shell.execute_reply": "2025-11-16T10:28:20.009234Z"
        },
        "id": "kRt9pugrWY2c"
      },
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PEFT techniques to the model - https://huggingface.co/docs/peft/en/package_reference/lora\n",
        "print(\"LORAAAAAAAAA\")\n",
        "#Build LoraConfig\n",
        "lora_config=LoraConfig(\n",
        "    r=config.lora_rank,\n",
        "    lora_alpha= config.lora_alpha,\n",
        "    target_modules = list(config.lora_target_modules),\n",
        "    lora_dropout = config.lora_dropout,\n",
        "    bias=\"none\",\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:28:25.704028Z",
          "iopub.execute_input": "2025-11-16T10:28:25.704307Z",
          "iopub.status.idle": "2025-11-16T10:28:25.708993Z",
          "shell.execute_reply.started": "2025-11-16T10:28:25.704289Z",
          "shell.execute_reply": "2025-11-16T10:28:25.708363Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLmReRrLWY2c",
        "outputId": "818b3752-bf6d-4f46-bbe7-a4e0dc3040a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LORAAAAAAAAA\n"
          ]
        }
      ],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "unet = get_peft_model(unet, lora_config)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:28:32.624184Z",
          "iopub.execute_input": "2025-11-16T10:28:32.624924Z",
          "iopub.status.idle": "2025-11-16T10:28:44.191106Z",
          "shell.execute_reply.started": "2025-11-16T10:28:32.624886Z",
          "shell.execute_reply": "2025-11-16T10:28:44.190507Z"
        },
        "id": "bSUl8LQ0WY2c"
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_after = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
        "total_after = sum(p.numel() for p in unet.parameters())\n",
        "print(f\"UNet trainable params after PEFT: {trainable_after:,} / {total_after:,} ({100*trainable_after/total_after:.4f}%)\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:28:47.343850Z",
          "iopub.execute_input": "2025-11-16T10:28:47.344122Z",
          "iopub.status.idle": "2025-11-16T10:28:47.376051Z",
          "shell.execute_reply.started": "2025-11-16T10:28:47.344105Z",
          "shell.execute_reply": "2025-11-16T10:28:47.375460Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1I0DwIqWY2c",
        "outputId": "b95b1a97-c271-4fab-fd4b-0df325676ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNet trainable params after PEFT: 83,722,240 / 1,534,862,084 (5.4547%)\n"
          ]
        }
      ],
      "execution_count": 38
    },
    {
      "cell_type": "code",
      "source": [
        "#oprtimizer and scheduler\n",
        "from transformers import get_scheduler"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:29:03.364510Z",
          "iopub.execute_input": "2025-11-16T10:29:03.365123Z",
          "iopub.status.idle": "2025-11-16T10:29:03.395375Z",
          "shell.execute_reply.started": "2025-11-16T10:29:03.365100Z",
          "shell.execute_reply": "2025-11-16T10:29:03.394664Z"
        },
        "id": "9O2bd8lTWY2d"
      },
      "outputs": [],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "source": [
        "device = acc.device\n",
        "print(\"Accelerator device:\", device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:29:15.163858Z",
          "iopub.execute_input": "2025-11-16T10:29:15.164115Z",
          "iopub.status.idle": "2025-11-16T10:29:15.168228Z",
          "shell.execute_reply.started": "2025-11-16T10:29:15.164098Z",
          "shell.execute_reply": "2025-11-16T10:29:15.167457Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3Vig4LZWY2d",
        "outputId": "9e283e03-80cb-4740-97e5-3527155fc9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accelerator device: cuda\n"
          ]
        }
      ],
      "execution_count": 40
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:29:31.923900Z",
          "iopub.execute_input": "2025-11-16T10:29:31.924673Z",
          "iopub.status.idle": "2025-11-16T10:29:31.928282Z",
          "shell.execute_reply.started": "2025-11-16T10:29:31.924650Z",
          "shell.execute_reply": "2025-11-16T10:29:31.927446Z"
        },
        "id": "_o78yCCeWY2e"
      },
      "outputs": [],
      "execution_count": 41
    },
    {
      "cell_type": "code",
      "source": [
        "if config.use_8bit_adam:\n",
        "  import bitsandbytes as bnb\n",
        "  optm = bnb.optim.AdamW8bit(unet.parameters(), lr=config.learning_rate)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:29:35.193935Z",
          "iopub.execute_input": "2025-11-16T10:29:35.194674Z",
          "iopub.status.idle": "2025-11-16T10:29:35.215017Z",
          "shell.execute_reply.started": "2025-11-16T10:29:35.194649Z",
          "shell.execute_reply": "2025-11-16T10:29:35.214332Z"
        },
        "id": "Q8JJz_hHWY2k"
      },
      "outputs": [],
      "execution_count": 42
    },
    {
      "cell_type": "code",
      "source": [
        "# Scheduler\n",
        "num_update_steps_per_epoch = math.ceil(len(train_loader) / config.gradient_accumulation_steps)\n",
        "max_train_steps = config.num_train_epochs * num_update_steps_per_epoch\n",
        "lr_scheduler = get_scheduler(\n",
        "    config.learning_rate_scheduler_type,\n",
        "    optimizer=optm,\n",
        "    num_warmup_steps=config.learning_rate_warmup_steps,\n",
        "    num_training_steps=max_train_steps\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:29:46.204468Z",
          "iopub.execute_input": "2025-11-16T10:29:46.205183Z",
          "iopub.status.idle": "2025-11-16T10:29:46.209145Z",
          "shell.execute_reply.started": "2025-11-16T10:29:46.205157Z",
          "shell.execute_reply": "2025-11-16T10:29:46.208355Z"
        },
        "id": "HmmUrBMhWY2k"
      },
      "outputs": [],
      "execution_count": 43
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoder_1.to(acc.device)\n",
        "text_encoder_2.to(acc.device)\n",
        "# safety: use same dtype as training\n",
        "text_encoder_1.to(torch.float16)\n",
        "text_encoder_2.to(torch.float16)\n",
        "text_encoder_1.eval()\n",
        "text_encoder_2.eval()\n",
        "vae.eval()\n",
        "for p in vae.parameters():\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:50:36.732405Z",
          "iopub.execute_input": "2025-11-16T10:50:36.733263Z",
          "iopub.status.idle": "2025-11-16T10:50:36.753106Z",
          "shell.execute_reply.started": "2025-11-16T10:50:36.733229Z",
          "shell.execute_reply": "2025-11-16T10:50:36.752532Z"
        },
        "id": "_y-F1c3MWY2k"
      },
      "outputs": [],
      "execution_count": 44
    },
    {
      "cell_type": "code",
      "source": [
        "def model_loss(unet_model, vae_model, te1, te2, noise_scheduler, batch):\n",
        "    device = acc.device\n",
        "    target_dtype = torch.float16 if config.mixed_precision == \"fp16\" else torch.float32\n",
        "\n",
        "    # 1. Input Prep\n",
        "    # Move pixels to GPU, but keep in float32 for the VAE\n",
        "    pixel_values = batch[\"pixel_values\"].to(device=device, dtype=torch.float32)\n",
        "    pixel_values = torch.nan_to_num(pixel_values, nan=0.0, posinf=1.0, neginf=0.0)\n",
        "    pixel_values = pixel_values.clamp(-1.0, 1.0)\n",
        "\n",
        "    input_ids_one = batch[\"input_ids_one\"].to(device)\n",
        "    input_ids_two = batch[\"input_ids_two\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 2.VAE Encoding\n",
        "        # Disable autocast to force the VAE to run in full float32 -> was giving main pain tbh\n",
        "        with torch.autocast(device.type, enabled=False, dtype=torch.float32):\n",
        "            enc = vae_model.encode(pixel_values)\n",
        "        #the above snippet will make sure that VAE runs on the GPU with float32!\n",
        "\n",
        "        if hasattr(enc, \"latent_dist\"):\n",
        "            latents = enc.latent_dist.sample()\n",
        "        elif isinstance(enc, torch.Tensor):\n",
        "            latents = enc\n",
        "        else:\n",
        "            raise RuntimeError(f\"Unexpected VAE.encode return type: {type(enc)}\")\n",
        "\n",
        "        # Latents are now on GPU in fp32.\n",
        "        # Scale and DOWNCAST them to fp16 for the UNet.\n",
        "        latents = (latents * vae_model.config.scaling_factor).to(dtype=target_dtype)\n",
        "        #scaled down using `latents*vae_model.config.scaling.factor` -> output is stll in float32\n",
        "        #target_dtype is float16 so .to() converts the above to that\n",
        "\n",
        "        # 3. text Encoding\n",
        "        # This will run in fp16 due to the accelerator's autocast\n",
        "        out1 = te1(input_ids_one, output_hidden_states=True)\n",
        "        out2 = te2(input_ids_two, output_hidden_states=True)\n",
        "\n",
        "        # penultimate hidden states\n",
        "        emb1 = out1.hidden_states[-2]\n",
        "        emb2 = out2.hidden_states[-2]\n",
        "\n",
        "        # correct pooled embedding\n",
        "        if hasattr(out2, \"text_embeds\"):\n",
        "            pooled_emb2 = out2.text_embeds\n",
        "        else:\n",
        "            pooled_emb2 = out2.hidden_states[-1][:, 0, :]\n",
        "            #pooled emb represents the sentecne as a whole and gives model a high level summary of the things\n",
        "\n",
        "        encoder_hidden_states = torch.cat([emb1, emb2], dim=-1)\n",
        "        #CLIP -> smaller model but efficent and OpenCLIP -> detailed and nuanced understanding of stuff --->> both get combined to give the model a better understanding of things\n",
        "\n",
        "    # Get batch size after VAE/Text encoding\n",
        "    bsz = latents.shape[0]\n",
        "\n",
        "    # 4. Noise and timesteps\n",
        "    noise = torch.randn_like(latents)\n",
        "    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device).long()\n",
        "    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "    if config.debug:\n",
        "        # quick checks\n",
        "        if encoder_hidden_states.shape[0] != bsz:\n",
        "            raise RuntimeError(\"Batch size mismatch between latents and text embeddings.\")\n",
        "        if torch.isnan(encoder_hidden_states).any():\n",
        "            raise RuntimeError(\"NaN detected in encoder_hidden_states.\")\n",
        "\n",
        "    # 5. Conditioning\n",
        "    # `add_time_ids` must match the dtype of the text embeddings (fp16)\n",
        "    add_time_ids = torch.tensor(\n",
        "        [[config.resolution, config.resolution, 0, 0, config.resolution, config.resolution]] * bsz,\n",
        "        device=device,\n",
        "        dtype=target_dtype # <-- CHANGED (from torch.long)\n",
        "    )\n",
        "    added_cond_kwargs = {\"text_embeds\": pooled_emb2, \"time_ids\": add_time_ids}\n",
        "\n",
        "    # UNet - forward pass\n",
        "    model_out = unet_model(noisy_latents, timesteps, encoder_hidden_states, added_cond_kwargs=added_cond_kwargs)\n",
        "    model_pred = model_out.sample if hasattr(model_out, \"sample\") else model_out\n",
        "\n",
        "\n",
        "    loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
        "\n",
        "    if config.debug and (torch.isnan(loss) or torch.isinf(loss)):\n",
        "\n",
        "        raise RuntimeError(\n",
        "            \"Loss became NaN/Inf. Debug dump: \"\n",
        "            f\"model_pred min/max = {float(model_pred.float().min()):.6f}/{float(model_pred.float().max()):.6f}, \"\n",
        "            f\"noise min/max = {float(noise.float().min()):.6f}/{float(noise.float().max()):.6f}\"\n",
        "        )\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:58:07.643641Z",
          "iopub.execute_input": "2025-11-16T10:58:07.643981Z",
          "iopub.status.idle": "2025-11-16T10:58:07.655673Z",
          "shell.execute_reply.started": "2025-11-16T10:58:07.643953Z",
          "shell.execute_reply": "2025-11-16T10:58:07.655037Z"
        },
        "id": "yWzX3RujWY2k"
      },
      "outputs": [],
      "execution_count": 45
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running single-batch dry-run to validate forward/backward steps...\")\n",
        "unet.train()\n",
        "batch = next(iter(train_loader))\n",
        "loss = model_loss(unet, vae, text_encoder_1, text_encoder_2, noise_scheduler, batch)\n",
        "print(\"Single-batch forward computed. Loss:\", float(loss.item()))\n",
        "\n",
        "acc.backward(loss)\n",
        "optm.step()\n",
        "optm.zero_grad()\n",
        "print(\"Backward + optimizer step carried out successfully (dry-run).\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-16T10:58:08.600871Z",
          "iopub.execute_input": "2025-11-16T10:58:08.601172Z",
          "iopub.status.idle": "2025-11-16T10:58:18.342274Z",
          "shell.execute_reply.started": "2025-11-16T10:58:08.601151Z",
          "shell.execute_reply": "2025-11-16T10:58:18.341486Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-isM8fUWY2l",
        "outputId": "6ee5e8e7-62f6-4d57-9d94-4d2467122934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running single-batch dry-run to validate forward/backward steps...\n",
            "Single-batch forward computed. Loss: 0.047874804586172104\n",
            "Backward + optimizer step carried out successfully (dry-run).\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running single-batch dry-run to validate forward/backward steps...\")\n",
        "\n",
        "unet.train()\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# --- THE FIX: Wrap EVERYTHING (Forward + Backward) in Autocast ---\n",
        "# This ensures that when Gradient Checkpointing re-runs layers during backward,\n",
        "# it still has the Autocast context to handle the FP16/FP32 mix.\n",
        "\n",
        "with torch.autocast(\"cuda\"):\n",
        "    # 1. Forward Pass\n",
        "    loss = model_loss(unet, vae, text_encoder_1, text_encoder_2, noise_scheduler, batch)\n",
        "    print(\"✅ Single-batch forward computed. Loss:\", float(loss.item()))\n",
        "\n",
        "    # 2. Backward Pass (Now inside the safety zone)\n",
        "    print(\"Running backward pass...\")\n",
        "    acc.backward(loss)\n",
        "\n",
        "# 3. Optimizer Step (Can be outside)\n",
        "optm.step()\n",
        "optm.zero_grad()\n",
        "\n",
        "print(\"✅ Backward + optimizer step carried out successfully (dry-run).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8JvPpSgGK5L",
        "outputId": "f301e3b7-4d4b-4518-84c3-3fa788ccf634"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running single-batch dry-run to validate forward/backward steps...\n",
            "✅ Single-batch forward computed. Loss: 0.026328371837735176\n",
            "Running backward pass...\n",
            "✅ Backward + optimizer step carried out successfully (dry-run).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "global_step = 0\n",
        "unet.train()\n",
        "\n",
        "print(\"Deslaab\")\n",
        "\n",
        "for epoch in range(config.num_train_epochs):\n",
        "\n",
        "    # nice progress bar\n",
        "    epoch_pbar = tqdm(\n",
        "        train_loader,\n",
        "        desc=f\"Epoch {epoch+1}/{config.num_train_epochs}\",\n",
        "        disable=not acc.is_main_process\n",
        "    )\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    steps_in_epoch = 0\n",
        "    t0 = time()\n",
        "\n",
        "    for step, batch in enumerate(epoch_pbar):\n",
        "\n",
        "        with acc.accumulate(unet):\n",
        "          with torch.autocast(\"cuda\"):\n",
        "            loss = model_loss(unet, vae, text_encoder_1, text_encoder_2, noise_scheduler, batch)\n",
        "\n",
        "            acc.backward(loss)\n",
        "\n",
        "            # Gradient clipping (safer)\n",
        "            acc.clip_grad_norm_(unet.parameters(), config.max_grad_norm)\n",
        "\n",
        "            optm.step()\n",
        "            lr_scheduler.step()\n",
        "            optm.zero_grad()\n",
        "\n",
        "        # Only log on sync steps (when optimizer actually stepped)\n",
        "        if acc.sync_gradients:\n",
        "            global_step += 1\n",
        "            steps_in_epoch += 1\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Print every 5 steps (adjust as needed)\n",
        "            if global_step % 5 == 0 and acc.is_main_process:\n",
        "                avg_loss = epoch_loss / steps_in_epoch\n",
        "                curr_lr = lr_scheduler.get_last_lr()[0]\n",
        "                step_time = (time() - t0) / max(1, steps_in_epoch)\n",
        "\n",
        "                epoch_pbar.set_postfix({\n",
        "                    \"loss\": f\"{avg_loss:.4f}\",\n",
        "                    \"lr\": f\"{curr_lr:.2e}\",\n",
        "                    \"step_time\": f\"{step_time:.2f}s\"\n",
        "                })\n",
        "\n",
        "            # Save checkpoint\n",
        "            if global_step % config.save_every_step == 0 and acc.is_main_process:\n",
        "                ckpt_path = f\"{config.checkpointsDir}/checkpoint-{global_step}\"\n",
        "                acc.save_state(ckpt_path)\n",
        "                print(f\"Saved checkpoint at: {ckpt_path}\\n\")\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    if acc.is_main_process:\n",
        "        print(f\"Finished Epochhhh {epoch+1}/{config.num_train_epochs} | \"\n",
        "              f\"Avg Loss: {epoch_loss/steps_in_epoch:.4f}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om-Gaa64GPyb",
        "outputId": "33709112-5b61-44cd-f5fb-9f3ac1735a37"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deslaab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:  82%|████████▏ | 1000/1221 [43:30<09:30,  2.58s/it, loss=0.1004, lr=2.00e-06, step_time=10.44s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at: /content/drive/MyDrive/Naruto_FineTune/checkpoints/checkpoint-250\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|██████████| 1221/1221 [53:00<00:00,  2.60s/it, loss=0.1014, lr=2.49e-05, step_time=10.42s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Epochhhh 1/3 | Avg Loss: 0.1014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3:  64%|██████▍   | 779/1221 [33:10<18:12,  2.47s/it, loss=0.0978, lr=9.12e-05, step_time=10.21s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at: /content/drive/MyDrive/Naruto_FineTune/checkpoints/checkpoint-500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 1221/1221 [52:07<00:00,  2.56s/it, loss=0.0937, lr=2.37e-05, step_time=10.24s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Epochhhh 2/3 | Avg Loss: 0.0937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3:  46%|████▌     | 558/1221 [23:38<27:13,  2.46s/it, loss=0.1056, lr=1.97e-05, step_time=10.13s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at: /content/drive/MyDrive/Naruto_FineTune/checkpoints/checkpoint-750\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 1221/1221 [51:54<00:00,  2.55s/it, loss=0.0980, lr=9.99e-05, step_time=10.18s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Epochhhh 3/3 | Avg Loss: 0.0980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "\n",
        "#Save\n",
        "final_save_path = \"/content/drive/MyDrive/Naruto_FineTune/final_weights\"\n",
        "os.makedirs(final_save_path, exist_ok=True)\n",
        "\n",
        "unet = acc.unwrap_model(unet)\n",
        "unet.save_pretrained(final_save_path)\n",
        "print(f\" Final weights saved safely to: {final_save_path}\")\n",
        "\n",
        "#Clean VRAM\n",
        "print(\"Cleaning up training models from VRAM...\")\n",
        "try:\n",
        "    del unet, text_encoder_1, text_encoder_2, vae, optm, acc\n",
        "except NameError:\n",
        "    print(\"Some models were already deleted.\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Training complete and LoRA weights saved.\")\n",
        "print(\"Memory cleared. Ready for inference.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL2FOZSjfJ6y",
        "outputId": "e5e11968-e28f-453b-a5ec-81e8b0b6a09e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Final weights saved safely to: /content/drive/MyDrive/Naruto_FineTune/final_weights\n",
            "Cleaning up training models from VRAM...\n",
            "Training complete and LoRA weights saved.\n",
            "Memory cleared. Ready for inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NYcKjrjpnDml"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}